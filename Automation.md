# ðŸ“‚ é¡¹ç›®ç›®å½•ç»“æž„å»ºè®®
YT_SUMMARIZER/
  â”œâ”€ yt_summarizer.py   # å·²æœ‰çš„æµæ°´çº¿è„šæœ¬ (è¾“å…¥: YouTube é“¾æŽ¥, è¾“å‡º: é‚®ä»¶)
  â”œâ”€ auto_runner.py     # è‡ªåŠ¨åŒ–æ£€æµ‹ + è°ƒç”¨ pipeline
  â”œâ”€ channels.json      # è¦è·Ÿè¸ªçš„é¢‘é“IDåˆ—è¡¨
  

# è¦è·Ÿè¸ªçš„é¢‘é“IDåˆ—è¡¨
channels.json 
{
  "channels": [
    "UCFQsi7WaF5X41tcuOryDk8w",  
    "UC2I5em6UyBpQiO-8ZW0nV3w",  
    "UCGDMLMZtjCd5P4fhTCetwsw",  
    "UCBUH38E0ngqvmTqdchWunwQ"
  ]
}



# ðŸ“ auto_runner.py
import feedparser
import subprocess
from datetime import datetime, timedelta, timezone
import json

# æ–‡ä»¶è·¯å¾„
CHANNELS_FILE = "channels.json"

# åŠ è½½é¢‘é“ID
with open(CHANNELS_FILE, "r") as f:
    channels = json.load(f)["channels"]

# èŽ·å–æ˜¨å¤©çš„æ—¶é—´èŒƒå›´ (UTCæ—¶é—´)
today = datetime.now(timezone.utc).date()
yesterday = today - timedelta(days=1)

new_videos = []

# éåŽ†é¢‘é“
for channel_id in channels:
    feed_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    feed = feedparser.parse(feed_url)

    for entry in feed.entries:
        # å‘å¸ƒæ—¶é—´ (è½¬æ¢ä¸º UTC æ—¥æœŸ)
        published = datetime.fromisoformat(entry.published.replace("Z", "+00:00")).date()

        if published == yesterday:
            video_id = entry.yt_videoid
            video_url = entry.link
            title = entry.title

            print(f"[NEW] {title} ({video_url}) å‘å¸ƒäºŽ {published}")
            new_videos.append((video_id, video_url))

# å¤„ç†æ–°è§†é¢‘
if new_videos:
    for video_id, video_url in new_videos:
        try:
            subprocess.run(["python3", "pipeline.py", video_url], check=True)
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {video_url}, é”™è¯¯: {e}")
else:
    print("âœ… æ˜¨å¤©æ²¡æœ‰æ–°è§†é¢‘ï¼Œè·³è¿‡ã€‚")


# ä½¿ç”¨æ–¹æ³•

å…ˆè¿è¡Œä¸€æ¬¡ï¼Œæ£€æŸ¥èƒ½å¦æ£€æµ‹åˆ°æ–°è§†é¢‘å¹¶è°ƒç”¨ pipeline.pyï¼š

python3 auto_runner.py


åŠ åˆ° cronï¼Œæ¯”å¦‚æ¯å¤©æ—©ä¸Š 9 ç‚¹è·‘ä¸€æ¬¡ï¼š

0 9 * * * /usr/bin/python3 /path/to/yt_auto/auto_runner.py >> /path/to/yt_auto/auto.log 2>&1
