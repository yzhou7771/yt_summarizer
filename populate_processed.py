#!/usr/bin/env python3
"""
å¡«å……ç°æœ‰è§†é¢‘åˆ°processed.json
"""

import os
import re
import json
from datetime import datetime

# è·å–å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶å¤¹
def get_existing_video_folders():
    """è·å–ç°æœ‰çš„è§†é¢‘æ–‡ä»¶å¤¹"""
    folders = []
    current_dir = '.'
    
    for item in os.listdir(current_dir):
        if os.path.isdir(item) and re.match(r'\d{4}_', item):
            folders.append(item)
    
    return sorted(folders)

def extract_video_info_from_folder(folder_name):
    """ä»æ–‡ä»¶å¤¹åç§°æå–è§†é¢‘ä¿¡æ¯"""
    # æ–‡ä»¶å¤¹æ ¼å¼ï¼šMMDD_é¢‘é“åç§°
    parts = folder_name.split('_', 1)
    if len(parts) != 2:
        return None
    
    date_part = parts[0]
    channel_name = parts[1]
    
    # å‡è®¾å¹´ä»½ä¸º2025å¹´ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    try:
        month = int(date_part[:2])
        day = int(date_part[2:])
        published_date = f"2025-{month:02d}-{day:02d}"
    except ValueError:
        return None
    
    return {
        'channel_name': channel_name,
        'published_date': published_date
    }

def populate_processed_videos():
    """å¡«å……å·²å¤„ç†çš„è§†é¢‘è®°å½•"""
    print("ğŸ”„ å¡«å……ç°æœ‰è§†é¢‘åˆ°processed.json")
    print("=" * 40)
    
    # åŠ è½½ç°æœ‰çš„processed.json
    processed_file = "processed.json"
    if os.path.exists(processed_file):
        with open(processed_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = {"processed_videos": {}}
    
    processed_videos = data["processed_videos"]
    
    # è·å–ç°æœ‰æ–‡ä»¶å¤¹
    folders = get_existing_video_folders()
    print(f"ğŸ“ å‘ç° {len(folders)} ä¸ªè§†é¢‘æ–‡ä»¶å¤¹")
    
    added_count = 0
    
    for folder in folders:
        print(f"\nğŸ“‚ å¤„ç†æ–‡ä»¶å¤¹: {folder}")
        
        # æå–è§†é¢‘ä¿¡æ¯
        info = extract_video_info_from_folder(folder)
        if not info:
            print(f"  âš ï¸ æ— æ³•è§£ææ–‡ä»¶å¤¹åç§°")
            continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰summary.txtæ–‡ä»¶
        summary_file = os.path.join(folder, "summary.txt")
        if not os.path.exists(summary_file):
            print(f"  âš ï¸ æ‰¾ä¸åˆ°summary.txtï¼Œè·³è¿‡")
            continue
        
        # è¯»å–summaryå†…å®¹è·å–è§†é¢‘æ ‡é¢˜
        try:
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary_content = f.read()
            
            # å°è¯•ä»æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹æ¨æ–­æ ‡é¢˜ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            video_title = f"è§†é¢‘æ¥è‡ª {info['channel_name']} - {info['published_date']}"
            
            # ç”Ÿæˆä¸€ä¸ªå”¯ä¸€IDï¼ˆåŸºäºæ–‡ä»¶å¤¹åï¼‰
            video_id = folder.replace('_', '-')
            
            # å¦‚æœè¿˜æ²¡æœ‰è®°å½•è¿™ä¸ªè§†é¢‘
            if video_id not in processed_videos:
                processed_videos[video_id] = {
                    "title": video_title,
                    "url": f"https://youtube.com/watch?v={video_id}",  # å ä½ç¬¦URL
                    "channel": info['channel_name'],
                    "published": info['published_date'],
                    "processed_at": "å·²å­˜åœ¨æ–‡ä»¶å¤¹",
                    "note": "ä»ç°æœ‰æ–‡ä»¶å¤¹å¯¼å…¥"
                }
                
                print(f"  âœ… æ·»åŠ è®°å½•: {video_title}")
                added_count += 1
            else:
                print(f"  â­ï¸ å·²å­˜åœ¨è®°å½•ï¼Œè·³è¿‡")
        
        except Exception as e:
            print(f"  âŒ å¤„ç†å¼‚å¸¸: {e}")
    
    # ä¿å­˜æ›´æ–°çš„è®°å½•
    if added_count > 0:
        with open(processed_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… æˆåŠŸæ·»åŠ  {added_count} ä¸ªè§†é¢‘è®°å½•")
        print(f"ğŸ“‹ æ€»å…±è®°å½•äº† {len(processed_videos)} ä¸ªå·²å¤„ç†è§†é¢‘")
    else:
        print(f"\nâœ… æ²¡æœ‰æ–°å¢è®°å½•ï¼Œæ€»å…± {len(processed_videos)} ä¸ªå·²å¤„ç†è§†é¢‘")

if __name__ == "__main__":
    populate_processed_videos()